{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Text Pre-processing"
      ],
      "metadata": {
        "id": "RktAyZH1nqpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing 'Key Words'"
      ],
      "metadata": {
        "id": "2sp-EmLbnxon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Removing '@names'"
      ],
      "metadata": {
        "id": "sQAQyXcDn04i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_pattern(text, pattern_regex):\n",
        "    r = re.findall(pattern_regex, text)\n",
        "    for i in r:\n",
        "        text = re.sub(i, '', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "4TQGJERvn5mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are keeping cleaned tweets in a new column called 'tidy_tweets'\n",
        "tweets_df['tidy_tweets'] = np.vectorize(remove_pattern)(tweets_df['tweets'], \"@[\\w]*: | *RT*\")\n",
        "tweets_df.head(10"
      ],
      "metadata": {
        "id": "WJi2DaDxn-1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Removing links (http | https)"
      ],
      "metadata": {
        "id": "2BLGd_jQoKIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets = []\n",
        "\n",
        "for index, row in tweets_df.iterrows():\n",
        "    # Here we are filtering out all the words that contains link\n",
        "    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n",
        "    cleaned_tweets.append(' '.join(words_without_links))\n",
        "\n",
        "tweets_df['tidy_tweets'] = cleaned_tweets\n",
        "tweets_df.head(10)"
      ],
      "metadata": {
        "id": "e6c0UEVJoDyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Removing tweets with empty text"
      ],
      "metadata": {
        "id": "to_biB3JoU-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = tweets_df[tweets_df['tidy_tweets']!='']\n",
        "tweets_df.head()"
      ],
      "metadata": {
        "id": "sf5LSE2MoQh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. Dropping duplicate rows"
      ],
      "metadata": {
        "id": "WJTxwkqOonxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.drop_duplicates(subset=['tidy_tweets'], keep=False)\n",
        "tweets_df.head()"
      ],
      "metadata": {
        "id": "gkvlqmp6oo49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Resetting index"
      ],
      "metadata": {
        "id": "KLZD3Du7ovZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = tweets_df.reset_index(drop=True)\n",
        "tweets_df.head()"
      ],
      "metadata": {
        "id": "o4ReJocMowXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f. Removing Punctuations, Numbers and Special characters"
      ],
      "metadata": {
        "id": "Wa9Z3tKfo41F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df['absolute_tidy_tweets'] = tweets_df['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")"
      ],
      "metadata": {
        "id": "v29I8Ueio5pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "g. Removing Stop words"
      ],
      "metadata": {
        "id": "vAg3ngAeo8R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set = set(stopwords)\n",
        "cleaned_tweets = []\n",
        "\n",
        "for index, row in tweets_df.iterrows():\n",
        "\n",
        "    # filerting out all the stopwords\n",
        "    words_without_stopwords = [word for word in row.absolute_tidy_tweets.split() if not word in stopwords_set and '#' not in word.lower()]\n",
        "\n",
        "    # finally creating tweets list of tuples containing stopwords(list) and sentimentType\n",
        "    cleaned_tweets.append(' '.join(words_without_stopwords))\n",
        "\n",
        "tweets_df['absolute_tidy_tweets'] = cleaned_tweets\n",
        "tweets_df.head(10)"
      ],
      "metadata": {
        "id": "yAlGS1eSo-_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "h. Tokenize *'absolute_tidy_tweets'*"
      ],
      "metadata": {
        "id": "9oCcMaBppLW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweet = tweets_df['absolute_tidy_tweets'].apply(lambda x: x.split())\n",
        "tokenized_tweet.head()"
      ],
      "metadata": {
        "id": "a4eP_YeWpMWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i. Converting words to Lemma"
      ],
      "metadata": {
        "id": "MHwV6q6wpWoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\n",
        "tokenized_tweet.head()"
      ],
      "metadata": {
        "id": "jiHmYBR_pXqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "j. Joining all tokens into sentences"
      ],
      "metadata": {
        "id": "ddDnax-YpdOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, tokens in enumerate(tokenized_tweet):\n",
        "    tokenized_tweet[i] = ' '.join(tokens)\n",
        "\n",
        "tweets_df['absolute_tidy_tweets'] = tokenized_tweet\n",
        "tweets_df.head(10)"
      ],
      "metadata": {
        "id": "X5bXqHwqpjLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}